---
title: "Lectura13"
author: "Byron Caices"
date: "2024-11-26"
output:
    html_document:
    highlight: tango
    word_document: default
    pdf_document: default
---

```{=html}
<style>
body {
  font-family: 'Calibri', sans-serif;
}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo =FALSE, warning=FALSE, message=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))

if (!requireNamespace('tidyverse', quietly = TRUE)){
  install.packages('tidyverse')
}
library(tidyverse)
if (!requireNamespace('ggpubr', quietly = TRUE)){
  install.packages('ggpubr')
}
library(ggpubr)
if (!requireNamespace('ez', quietly = TRUE)){
  install.packages('ez')
}
library(ez)
if (!requireNamespace('RVAideMemoire', quietly = TRUE)){
  install.packages('RVAideMemoire')
}
library(RVAideMemoire)
if (!requireNamespace('rcompanion', quietly = TRUE)){
  install.packages('rcompanion')
}
library(rcompanion)
if (!requireNamespace('dplyr', quietly = TRUE)){
  install.packages('dplyr')
}
library(dplyr)
if (!requireNamespace('WRS2', quietly = TRUE)){
  install.packages('WRS2')
}
library(WRS2)
```

# Regresión Lineal Simple

## Coeficiente de Correlacion de Pearson R

Puede ir entre -1 y 1. Si es 0 no hay correlación. Si R>0 la correlación es directa y si R<0 la correlación es inversa.

Pensando en las muestras de dos variables, el coeficiente de correlación de Pearson queda dado por la siguiente ecuación:

\[
R = \frac{1}{n - 1} \sum_{i=1}^{n} \frac{(x_i - \bar{x})}{s_x} \cdot \frac{(y_i - \bar{y})}{s_y}
\]

donde:

- \(\bar{x}\), \(\bar{y}\) son las medias de las variables \(X\) e \(Y\) en la muestra.
- \(s_x\), \(s_y\) corresponden a las desviaciones estándar de las variables \(X\) e \(Y\) en la muestra.
- \(n\) es el tamaño de la muestra.

---------

La función `cor()` en R se utiliza para calcular la correlación entre dos variables o para generar una **matriz de correlación** cuando se trabaja con múltiples variables.

### Uso básico:
- **`cor(x, y)`**: Calcula el coeficiente de correlación entre dos vectores numéricos \(x\) e \(y\), donde \(x\) es el predictor y \(y\) la respuesta.
- Los resultados son valores entre \(-1\) y \(1\), donde:
  - \(1\): Correlación perfecta positiva.
  - \(-1\): Correlación perfecta negativa.
  - \(0\): No hay correlación.

### Matriz de correlación:

- Si se aplica a una matriz o un data frame con variables numéricas, como `cor(x)`, devuelve una **matriz de correlación** con los coeficientes para todas las combinaciones de pares de columnas.
- La matriz es simétrica y su diagonal contiene únicamente valores de 1, ya que la correlación de una variable consigo misma es perfecta.

Ejemplo práctico:
```{r}
# Correlación entre dos variables
cor(mtcars$mpg, mtcars$hp)

# Matriz de correlación para todo el conjunto de datos
cor(mtcars)
```

Esta función es útil para el análisis exploratorio de datos, ayudando a identificar relaciones lineales entre variables.

```{r ajuste de una regresión lineal simple}

datos <- mtcars %>% filter(wt > 2 & wt < 5)

# Ajustar modelo con R
modelo <- lm(hp ~ disp, data = datos)

print(summary(modelo))

# Graficar los datos y el modelo obtenido 
g1 <- ggscatter(datos, x = "disp", y = "hp",
                color = "steelblue", fill = "steelblue",
                ylab = "Potencia [hp]")
g1 <- g1 + geom_abline(intercept = coef(modelo)[1],
                       slope = coef(modelo)[2],
                       color = "red")
g1 <- g1 + xlab(bquote("Volumen util de los cilindros" ~ group("[", "in"^3, "]")))
print(g1)

```
`Coefficients` 

```{r Predecir datos con modelo creado}
# Definir valores del predictor para vehículos no incluidos
# en el conjunto mtcars
disp <- c(169.694, 230.214, 79.005, 94.085, 343.085,
          136.073, 357.305, 288.842, 223.128, 129.217,
          146.432, 193.474, 376.874, 202.566, 114.928)

# Usar el modelo para predecir el rendimiento de estos modelos.
potencia_est <- predict(modelo, data.frame(disp))

# Graficar los valores predichos
nuevos <- data.frame(disp, hp = potencia_est)

g2 <- ggscatter(nuevos, x = "disp", y = "hp",
                color = "purple", fill = "purple",
                ylab = "Potencia [hp]")
g2 <- g2 + xlab(bquote("Volumen util de los cilindros" ~ group("[", "in"^3, "]")))

# Unir los gráficos en uno solo
g1 <- ggpar(g1, xlim = c(75, 405), ylim = c(60, 340))
g2 <- ggpar(g2, xlim = c(75, 405), ylim = c(60, 340))
g <- ggarrange(g1, g2,
               labels = c("Modelo", "Predicciones"),
               hjust = c(-1.2, -0.7))
print(g)

```

## Confiabilidad de un modelo de RLS

### Bondad de Ajuste (R^2, F y p-value)

No hemos revisado si el modelo conseguido cumple con todas las condiciones para utulizar RLS o si representa bien los datos observados.

Coeficiente de determinacion aka R-squared

F: evalúa si la reduccion en la varianza debido al modelo de regresión planteado es estadísticamente significativa. En este caso se le asocia un p-value el cual si es menor al alpha establecido entonces se concluye que el modelo ajustado mejora significativamente respecto al modelo nulo reforzando la idea de un ajuste adecuado.

### Distribución e independencia

Condiciones que se deben observar en el **gráfico de residuos** para aplicar RLS

1. Se distribuyen aleatoreamente en torno a la linea de valor cero

2. Forman una banda horizontal en torno a la línea de valor cero

3. No hay reisduos que se alejen del patrón que forman los demás

4. No forman un patrón reconocible

### **Interpretación de las pruebas y sus hipótesis:**

#### 1. **Prueba de curvatura (`residualPlots`):**

   - **Hipótesis nula (\( H_0 \)):** No existe curvatura en la relación entre los residuos y las variables predictores. Esto implica que el modelo lineal es adecuado.
   - **Hipótesis alternativa (\( H_A \)):** Existe curvatura en la relación entre los residuos y las variables predictores. Esto indica que el modelo lineal puede no ser adecuado.
   - **Resultado:**
     - \( p = 0.709 \) para el predictor `disp` y \( p = 0.7054 \) para la prueba de Tukey.
     - Ambos \( p \)-valores son mayores que el nivel de significancia típico (\( \alpha = 0.05 \)), por lo que no hay evidencia suficiente para rechazar \( H_0 \). Esto sugiere que no se detecta curvatura y que el modelo lineal es adecuado.

---

#### 2. **Prueba de independencia (`durbinWatsonTest`):**

   - **Hipótesis nula (\( H_0 \)):** Los residuos son independientes (no hay autocorrelación).
   - **Hipótesis alternativa (\( H_A \)):** Los residuos no son independientes (hay autocorrelación).
   - **Resultado:**
     - \( p = 0.102 \), mayor que \( \alpha = 0.05 \).
     - No se rechaza \( H_0 \), lo que indica que no hay evidencia significativa de autocorrelación en los residuos. Esto sugiere que los residuos son independientes.

---

#### 3. **Prueba de homocedasticidad (`ncvTest`):**

   - **Hipótesis nula (\( H_0 \)):** Los residuos tienen varianza constante (homocedasticidad).
   - **Hipótesis alternativa (\( H_A \)):** Los residuos no tienen varianza constante (heterocedasticidad).
   - **Resultado:**
     - \( p = 0.027 \), menor que \( \alpha = 0.05 \).
     - Se rechaza \( H_0 \), lo que indica que hay evidencia significativa de heterocedasticidad. Esto sugiere que los residuos no tienen varianza constante y que una suposición clave del modelo lineal no se cumple.

---

#### Conexión de las pruebas con las condiciones:

1. Distribución aleatoria:	residualPlots
2. Banda horizontal:	      ncvTest
3. Sin valores atípicos:	  residualPlots, marginalModelPlots
4. Sin patrones:        	  durbinWatsonTest

En resumen, estas pruebas son fundamentales para validar las suposiciones de un modelo de regresión lineal simple. Asegurarse de que las condiciones se cumplan es esencial para garantizar que el modelo sea fiable y que las inferencias sean válidas.



```{r Distribucion e independencia}
library(car)
library(dplyr)
library(ggpubr)

datos <- mtcars %>% filter(wt > 2 & wt < 5)

modelo <- lm(hp ~ disp, data = datos)

cat("Pruebas de curvatura: \n")

# Desplegar gráficos de residuos y mostrar pruebas de curvatura
residualPlots(modelo, type = "rstandard",
              id = list(method = "r", n = 3, cex = 0.7, location = "lr"),
              col = "steelblue", pch = 20, col.quad = "red")

# Verificar independencia de los residuos
set.seed(19) # fijamos semilla porque durbinWatson utiliza bootstrapping
db <- durbinWatsonTest(modelo)
cat("\nPrueba de independencia:\n")
print(db)

# Desplegar gráficos marginales
marginalModelPlots(modelo, sd = TRUE,
                   id = list(method = "r", n = 3, cex = 0.7, location = "lr"),
                   col = "steelblue", pch = 20, col.line = c("steelblue","red"))

# Prueba de la varianza del error no constante
cat("\nPurbea de homocedasticidad:\n")
print(ncvTest(modelo))

# Desplegar graficos de influencia
casos_influyentes <- influencePlot(modelo, id = list(cex = 0.7))
cat("\nCasos que podrían ser influyentes:\n")
print(casos_influyentes)
```

Nos damos cuenta de que entonces la homocedasticidad no se está cumpliendo. 

### Influencia de valores atípicos

1. Apalancamiento/leverage: Valor relacionado a que tan lejos se encuentra un valor x_i respecto a la media muestral. Estos valores son potenciales puntos influyentes

- Obs con apalancamiento 2 o más veces mayor al valor promedio son casos que podrian ser influyentes pero eso no siempre implica un problema.

Otra forma de verificar puntos influyentes es mediante la **Distancia de Cook** con el gráfico influencePlot

### Calidad Predictiva de un modelo de RLS

#### Error de un modelo de RLS

- El error mide qué tan lejos estás de la realidad.
- MSE y RMSE son formas de resumir qué tan malo o bueno es tu modelo en promedio.
- RMSE es más fácil de interpretar porque está en la misma escala que los datos (como HP en este caso).
- Mientras más chico el RMSE, mejor es tu modelo para predecir.

#### Generalizacion de un modelo

Un modelo es generalizable si para un conjunto de datos nuevo consigue predicciones con una calidad similar al que consigue con los datos usado en su construccion.

**1. Validacion Cruzada:** Conjunto de datos se separa en conjunto de entrenamiento y conjunto de prueba donde el primero se usa paraa ajustar la recta y el segundo para evaluar el modelo con datos nuevos

```{r Cross Validation}

datos <- mtcars %>% filter(wt > 2 & wt < 5)
n <- nrow(datos) 

set.seed(101)
n_entrenamiento <- floor(0.8 * n) # 80% de datos para entrenar
i_entrenamiento <- sample.int(n = n, size = n_entrenamiento, replace = FALSE)
entrenamiento <- datos[i_entrenamiento,]
prueba <- datos[-i_entrenamiento,] # Excluye datos con el indice seleccionado para entrenamiento

# Ajustar y mostrar el modelo con el conjunto de entrenamiento

# dependiente ~ independiente
modelo <- lm(hp ~ disp, data = entrenamiento)
print(summary(modelo))

# Calcualar el error cuadrado promedio para el conjunto de entrenamiento

rmse_entrenamiento <- sqrt(mean(resid(modelo) ** 2 ))
cat("RMSE para el conjunto de entrenamiento:", rmse_entrenamiento, "\n")

# Hacer predicciones para el conjunto de prueba
predicciones <- predict(modelo, prueba) # paso modelo y datos de prueba
# predicciones es un vector con los valores de hp predichos

# Calcular error cuadrado promedio para el conjunto de prueba
error <- prueba[["hp"]] - predicciones
rmse_prueba <- sqrt(mean(error ** 2))
cat("RMSE para ek conjunto de prueba:", rmse_prueba)
```

Notamos que los valores de RMSE son similares lo que sugiere quye el modelo conseguido generaliza bien otros datos distintos a los de su entrenamiento

Pero podría ocurrir que justo el conjunto de prueba quede con observaciones que NO representan adecuadamente la muestra de datos original

Para esto tenemos...
 
**1. Validacion Cruzada de k pliegues:** 

1. Divides tus datos en \(k\) grupos o pliegues de igual tamaño.
2. Para cada pliegue:
   - Tomas un pliegue como conjunto de prueba.
   - Usas los otros \(k-1\) pliegues como conjunto de entrenamiento para ajustar el modelo.
   - Calculas el error en el conjunto de prueba.
3. Repites el proceso \(k\) veces, cambiando el pliegue que actúa como prueba en cada iteración.
4. Al final, calculas el promedio de los errores obtenidos en las \(k\) iteraciones.
5. El resultado es una estimación más confiable del desempeño del modelo, evitando que la evaluación dependa únicamente de un único conjunto de datos.

```{r k-Cross Validation}
library(caret)
library(dplyr)

# traincontrol es de caret
datos <- mtcars %>% filter(wt > 2 & wt < 5)
n <- nrow(datos) 

# Ajustar y mnostrar el modelo usando validacion cruzada de 5 pliegues
set.seed(111)
entrenamiento <- train(hp ~ disp, data = datos, method = "lm",
                       trControl = trainControl(method = "cv", number = 5))
# Divide los datos en 5 grupos iguales, entrena el modelo en 4 de ellos y lo prueba en el restante repitiendo el proceso 5 veces

modelo <- entrenamiento[["finalModel"]]
print(summary(modelo))

# Mostrar los resultados de cada pliegue
cat("Errores en cada pliegue:\n")
print(entrenamiento[["resample"]])

# Mostrar el resultado estimado para el modelo
cat("Error estimado para el modelo:\n")
print(entrenamiento[["results"]])
```

RMSE: Promedio de los errores del modelo.
RMSE_SD: Qué tan diferentes son los errores entre los pliegues.
Si RMSE_SD es alto, deberías investigar qué causa esas variaciones y mejorar tu modelo.

```{r LOOCV k equals n}

library(caret)
library(dplyr)

# traincontrol es de caret
datos <- mtcars %>% filter(wt > 2 & wt < 5)
n <- nrow(datos) 

# Ajustar y mnostrar el modelo usando validacion cruzada
set.seed(111)
entrenamiento <- train(hp ~ disp, data = datos, method = "lm",
                       trControl = trainControl(method = "LOOCV"))

modelo <- entrenamiento[["finalModel"]]
print(summary(modelo))

# Mostrar los errores
cat("Predicciones en cada pliegue:\n")
print(entrenamiento[["pred"]])

# Mostrar el resultado estimado para el modelo
cat("\nError estimado par el modelo:\n")
print(entrenamiento[["results"]])


```
